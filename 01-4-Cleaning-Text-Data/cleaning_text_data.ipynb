{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Manual\n",
    "\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis,', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook,', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'Title:', 'Metamorphosis', 'Author:', 'Franz', 'Kafka', 'Translator:', 'David', 'Wyllie', 'Release', 'Date:', 'August', '16,', '2005', '[EBook', '#5200]', 'First', 'posted:', 'May', '13,', '2002', 'Last', 'updated:']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])\n",
    "\n",
    "# This still isn't perfect as the results would have outputs like this: \"wasn't\", \"dream.\", \"room,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated']\n"
     ]
    }
   ],
   "source": [
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])\n",
    "\n",
    "# Now we got, \"wasn\" and \"t\" from \"wasn't\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'wwwgutenbergnet', '', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', '', '', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:100])\n",
    "\n",
    "# Finally we got, \"wasnt\" from \"wasn't\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis,', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie.', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'you', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook,', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'title:', 'metamorphosis', 'author:', 'franz', 'kafka', 'translator:', 'david', 'wyllie', 'release', 'date:', 'august', '16,', '2005', '[ebook', '#5200]', 'first', 'posted:', 'may', '13,', '2002', 'last', 'updated:']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])\n",
    "\n",
    "# This works well too but, like “US” to “us”, can change meanings when reduced to the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\suwadith\\appdata\\local\\enthought\\canopy\\edm\\envs\\user\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\suwadith\\appdata\\local\\enthought\\canopy\\edm\\envs\\user\\lib\\site-packages (from nltk) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in c:\\users\\suwadith\\appdata\\local\\enthought\\canopy\\edm\\envs\\user\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "!pip install -U nltk\n",
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Suwadith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', ',', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', '.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', ',', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'Title', ':', 'Metamorphosis', 'Author', ':', 'Franz', 'Kafka', 'Translator', ':', 'David', 'Wyllie', 'Release', 'Date', ':']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# It does the same thing as split() we saw above.\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens[:100])\n",
    "\n",
    "# We can see  'looked', '.', '``', 'What', \"'s\" in this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\nTranslated by David Wyllie.', 'This eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.', 'You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.net\\n\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\n**     Please follow the copyright guidelines in this file.', '**\\n\\n\\nTitle: Metamorphosis\\n\\nAuthor: Franz Kafka\\n\\nTranslator: David Wyllie\\n\\nRelease Date: August 16, 2005 [EBook #5200]\\nFirst posted: May 13, 2002\\nLast updated: May 20, 2012\\n\\nLanguage: English\\n\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\n\\n\\n\\n\\nCopyright (C) 2002 David Wyllie.', 'Metamorphosis\\n  Franz Kafka\\n\\nTranslated by David Wyllie\\n\\n\\n\\nI\\n\\n\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.', 'He lay on\\nhis armour-like back, and if he lifted his head a little he could\\nsee his brown belly, slightly domed and divided by arches into stiff\\nsections.', 'The bedding was hardly able to cover it and seemed ready\\nto slide off any moment.', 'His many legs, pitifully thin compared\\nwith the size of the rest of him, waved about helplessly as he\\nlooked.', '\"What\\'s happened to me?\"', 'he thought.']\n"
     ]
    }
   ],
   "source": [
    "# You may not feel a big difference just by looking at this, but you can easily handle the split by sentences using NLTK. \n",
    "# Let’s replace word_tokenizer with sent_tokenizer\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n",
    "\n",
    "# If you don’t like regex, I think you’ll be very happy to see this.\n",
    "# It provides a high-level api, it can be easily handled by a function called isalpha()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Suwadith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "# We can easily get result\n",
    "# ['dreams', 'found', 'transformed', 'bed', 'horrible'] \n",
    "# from ['dreams', 'he', 'found', 'himself', 'transformed'] \n",
    "# using set(stopwords.words('english'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosi', ',', 'by', 'franz', 'kafka', 'translat', 'by', 'david', 'wylli', '.', 'thi', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyon', 'anywher', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrict', 'whatsoev', '.', 'you', 'may', 'copi', 'it', ',', 'give', 'it', 'away', 'or', 're-us', 'it', 'under', 'the', 'term', 'of', 'the', 'project', 'gutenberg', 'licens', 'includ', 'with', 'thi', 'ebook', 'or', 'onlin', 'at', 'www.gutenberg.net', '**', 'thi', 'is', 'a', 'copyright', 'project', 'gutenberg', 'ebook', ',', 'detail', 'below', '**', '**', 'pleas', 'follow', 'the', 'copyright', 'guidelin', 'in', 'thi', 'file', '.', '**', 'titl', ':', 'metamorphosi', 'author', ':', 'franz', 'kafka', 'translat', ':', 'david', 'wylli', 'releas', 'date', ':']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])\n",
    "\n",
    "# Stemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, \n",
    "# usually a suffix. There are several stemming models, including Porter and Snowball. \n",
    "# But there is a danger of “over-stemming” were words like “universe” \n",
    "# and “university” are reduced to the same root of “univers”.\n",
    "\n",
    "# After stemming, ['morning', 'troubled'] converted to ['morn','troubl'] by PorterStemmer.\n",
    "# You can also see that the tokens converted to lowercase.\n",
    "# Finally, we have cleaned text data reducing words to their root by stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other tools\n",
    "# 1. Lemmatization\n",
    "# Lemmatization is also an alternative to removing inflection. \n",
    "# By determining the part of text and utilizing WordNet’s lexical database of English, it can get better results.\n",
    "\n",
    "# It is a more accurate but slower. \n",
    "# Stemming may be more useful in queries for databases whereas Lemmatization may work much better when trying to \n",
    "# determine text sentiment.\n",
    "\n",
    "# 2. Word Embedding/Text Vectors\n",
    "# Word embedding is the modern way of representing words as vectors. \n",
    "# The aim of word embeddings is to find a series of high dimensionality vectors (one for each word) that represent \n",
    "# the relation of words in such a way that semantically related words are ‘close together’ in that high dimensional space. \n",
    "# Word2Vec and GloVe are the most common models for converting text to vectors. Often, T-SNE (as well as PCA) is used to \n",
    "# reduce the dimensionality enough to display as a 2 or 3 dimensional graph. Check out this example of T-SNE applied to \n",
    "# word embeddings.\n",
    "\n",
    "# Further Reading\n",
    "# Shubham Jain’s post(Ultimate guide to deal with Text Data\n",
    "\n",
    "# In that article you can learn different feature extraction methods, \n",
    "# starting with some basic techniques which will lead into advanced Natural Language Processing techniques including \n",
    "# <code>N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency (TF-IDF),  \n",
    "# Bag of Words and Sentiment Analysis</code>\n",
    "\n",
    "# In addition, if you want to dive deeper, visit video course on NLP (using Python).\n",
    "\n",
    " \n",
    "\n",
    "# Summary\n",
    "# In this article, you saw what cleaning text is and looked into it in Python codes.\n",
    "\n",
    "# Specifically, you learned it by 4 steps\n",
    "\n",
    "# Sneak peek into the data\n",
    "# Whitespace/Punctuation/Normalize Case\n",
    "# Stopwords/Stemming\n",
    "# Other tools\n",
    "# Also, I think you have understood the pros of using NLTK compared to manually implementing it.\n",
    "\n",
    "# Cleaning text can be performed variously depending on the what the purpose is. \n",
    "# It would be nice to study the methods that I haven’t introduced today at Futher Reading."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled29.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
